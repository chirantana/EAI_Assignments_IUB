# a3-report

Both programs were tested on SILO.

-Part 1

For this problem, it is separated into the three main models as illustrated in the assignment. We figured that problems a and b would be easiest since we have implemented similar programs to both of those in the past. Problem c was a bit more compicated and took a lot more though to get a working answer. We will discuss each problem in the upcoming paragraphs.

To start, we had to first set up the emission, transition, and initial probabilities. To get the initial probabilities, we just found the percentage of times that each POS was at the beginning of the sentences in the training set. SImilarly for transitions, we found the percentage of times that and given POS is followed by another POS for each part of speech. FInally,, for emissions, we found the probability that each given word will be a particular POS for all POS types. Each of these tables were created by just reading through all of the training sentences and were used as approximate estimates when testing our data and getting results.

We then set off to actualy build the models. For problem 1, we just had to create the simplified version of the HMM model. We noted first that there were no transition probabilities at all since there was no dependance between POS's in this model. This means that we can entirely ignore that table for this problem. For each sentence, the first word is predicted by using the argmax or the initial probability of a POS multiplied by the probability that a word is a specified POS. For every other sentence, the predicted POS is simply the argmax of the probabilities that a word is a specific POS. This essentially means that we just took what POS a specified word most belonged to and made that the prediction. We found that by using this method, about 91.76% of words and 37.75% of sentences were labeled correctly.

For problem 2, we just implemented Viterbi to solve it. We used the solution to the Viterbi coding assignment that we did in class a few weeks ago as the baseline to write our code. We adapted it to our needs by using our probability tables and got what seems to be a very good result. Here, we found that this method classified 94.27% of words and 50.20% of sentences correctly. These results were notably better than the ones found by using to simplified model.

For problem 3, we ran into more issues. After working at it for a while with no solid results, we ended up attending office hours on Tuesday morning for assistance with understanding the problem. The AI at office hours helped to point us in the right direction for this problem. Per his advice, we took several thousand samples from the training set and initialized all of thier POS labels to an arbitrary POS value. From there, we started learning each words representation given all other labels in the set. We started at the first word and moved forward in the sentenece one word at a time until all of the words had a prediction. The interesting thing that we noted was that the first prediction from our algorithms did not change during subsequent time steps. We found that this was because the model is only dependand on the values of words behind it so, once we had precition for the first k words, the prediction for word k+1 would be dependant only on the word immediatly behind it. FUrthermore, since the first word was only based off of initial and emission probability, it does not change during different time steps. Ultimately this does seem to work well as we end up getting an accuracy of about 92.42% for words and 39.35% on sentences. This is slightly lower than the HMM but still excelent results. These results do get slightly better the more samples that are used.

Finally, we had to calculate the posterior probabilities. These were found by multipliying probabilities for words one at a time based off of the given model and then taking the log of the results. We did also decide not to put 0's in most places since it did cause errors where it would entirely get rid of a value when multiplied, or could cause division errors or problems with the log in some cases. Instead, we chose to use a constant the we called E to represent a really small number (10^-50) instead of 0. This did not negatively effect the algorithms performance and got rid of errors caused by using 0.

-Part 2

For this part, the problem formulation was very similar to the first one. The differences are that we are trying to predict english letters given a matrix of pixels. This means that, instead of having transitions and intial probabilities based of the words of a sentenece like in part 1, we had transitions between letters in this part. Thus, we had to design tables for these new types of transitions and intial characteristics. Note that we did give these values a slightly smaller weight as compared to the emission probabilities. This is because we decided that the best way to determine what a letter is is to just look at it. Contaxt with letters is important, but it should not outweigh visual cues. We trained these arrays by using the same bc.train file as in problem 1. 

For the emission probabilities, we had to use something a bit different. Instead of building a pre-computed table like in part 1, we stored the training images for each latter and directly compared our testing images to them. For every pixel that was the same between images, we added a value to the score. And for every pixel that was not the same, we did not add anything. For pixels that were both black, we gave a higher weight compared to pixels that were both white. This is because we believe that the most important thing in this problem was to find where the letters were, rather than where they weren't. However, the latter does still need some weight in order to recognize spaces. Finally, we divided this score by the total number of pixels in order to regularize the data. We calculated these probabilities for every letter in the sentenece compared to every possible letter in order to get our emission probabilities that were used in the algorithms.

As far as the actual algorithms go, we just used the same code from part one with small modifications for the variables. However, since we were careful to write the general algorithm in part 1, this part was mostly copy-and-paste will very little change between the problems.

Overall, we found that both algorithms perform relatively well. The simple model does make more mistakes. In particular, it struggles to differentiate between "I", "l", and "1" and will often predict the wronge one. This is resolved with the HMM model because, when the letters look similar, it takes the context from the last letter to figure out what the best prediction would be.
